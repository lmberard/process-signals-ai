{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83e21c9",
   "metadata": {},
   "source": [
    "# # TP10: Multinomial Naive Bayes\n",
    "\n",
    "**Alumna**: Lucia Berard\n",
    "\n",
    "**Fecha**: 15/06/2025\n",
    "\n",
    "[Link a Google Colab](https://colab.research.google.com/drive/1ah4GT2vGlptvJM9qctPRs0NpqqozR07z?usp=sharing)\n",
    "\n",
    "\n",
    "La base de datos *BuzzFeed-Webis Fake News Corpus 2016* posee diferentes artículos periodísticos de una semana cercana a las elecciones estadounidenses de ese año. Se desea entrenar un algoritmo Multinomial Naive Bayes capaz de clasificar los artículos en: “mayormente falso”, “mayormente verdadero”, “mezcla de verdadero y falso” y “sin contenido factual”.\n",
    "\n",
    "![Banner UNICEF Argentina](https://www.unicef.org/argentina/sites/unicef.org.argentina/files/styles/media_banner/public/3%20%282%29.webp?itok=czU8qvE-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dabfe5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 1.6.1\n",
      "numpy: 2.0.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print('sklearn:', sklearn.__version__)\n",
    "print('numpy:', np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d1ed4",
   "metadata": {},
   "source": [
    "____\n",
    "## (a) Exploración de datos:\n",
    "\n",
    "#### a.1) Descargar la base de datos en https://zenodo.org/record/1239675/files/articles.zip?download=1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38d25e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "Archive:  articles.zip\n"
     ]
    }
   ],
   "source": [
    "!wget -nc 'https://zenodo.org/record/1239675/files/articles.zip?download=1' -O articles.zip\n",
    "!unzip -n articles.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c4e4b",
   "metadata": {},
   "source": [
    "\n",
    "#### a.2) Construir la base de datos. Puede usar el siguiente código:\n",
    "\n",
    "```python\n",
    "import xml.etree.ElementTree as ET\n",
    "data = {\"mainText\": [], \"orientation\": [], \"veracity\": []}\n",
    "\n",
    "for filename in os.listdir(\"articles/\"):\n",
    "    root = ET.parse(f\"articles/{{filename}}\").getroot()\n",
    "    for elem in root:\n",
    "        if elem.tag in data.keys():\n",
    "                data[elem.tag].append(elem.text)\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    data = data[data.notna().all(axis=\"columns\")]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "690a2cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mainText</th>\n",
       "      <th>orientation</th>\n",
       "      <th>veracity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Millions of people tuned in Monday night to wa...</td>\n",
       "      <td>left</td>\n",
       "      <td>mostly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Clintons understand the average American. ...</td>\n",
       "      <td>right</td>\n",
       "      <td>mixture of true and false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harassment is known in Arabic as ‘taharrush’. ...</td>\n",
       "      <td>right</td>\n",
       "      <td>mixture of true and false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Democratic President Barack Obama pulled off a...</td>\n",
       "      <td>left</td>\n",
       "      <td>mostly true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democratic nominee Hillary Clinton knows her f...</td>\n",
       "      <td>left</td>\n",
       "      <td>mostly true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            mainText orientation  \\\n",
       "0  Millions of people tuned in Monday night to wa...        left   \n",
       "1  The Clintons understand the average American. ...       right   \n",
       "2  Harassment is known in Arabic as ‘taharrush’. ...       right   \n",
       "3  Democratic President Barack Obama pulled off a...        left   \n",
       "4  Democratic nominee Hillary Clinton knows her f...        left   \n",
       "\n",
       "                    veracity  \n",
       "0                mostly true  \n",
       "1  mixture of true and false  \n",
       "2  mixture of true and false  \n",
       "3                mostly true  \n",
       "4                mostly true  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\"mainText\": [], \"orientation\": [], \"veracity\": []}\n",
    "\n",
    "for filename in os.listdir(\"articles/\"):\n",
    "    root = ET.parse(f\"articles/{filename}\").getroot()\n",
    "    for elem in root:\n",
    "        if elem.tag in data.keys():\n",
    "            data[elem.tag].append(elem.text)\n",
    "\n",
    "# Convertir a DataFrame y eliminar filas con valores nulos\n",
    "df = pd.DataFrame(data)\n",
    "df = df[df.notna().all(axis=\"columns\")]\n",
    "\n",
    "print(\"Primeras filas del DataFrame:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6e449",
   "metadata": {},
   "source": [
    "\n",
    "#### a.3) Utilice el comando `train_test_split` (sklearn) para definir dos conjuntos de datos. El conjunto de entrenamiento debe contener el 80% de las muestras, el resto será de testeo.\n",
    "\n",
    "Para definir el 80% de las muestras, utilice el parámetro `test_size=0.2` que indica el porcentaje de muestras que se utilizarán para el testeo.\n",
    "\n",
    "#### a.4) Utilizando `CountVectorizer` (sklearn) pre-procesar los datos del texto principal de los artículos. Se recomienda convertir el texto a minúscula, utilizar como `stop_words` las palabras estándar del idioma inglés, eliminar las palabras que aparecen en más del 60% de los documentos y descartar las palabras vistas en menos de 3 documentos.\n",
    "\n",
    "[`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de `sklearn` convierte una colección de documentos de texto en una matriz de conteo de palabras. Esto significa que toma un conjunto de textos y los transforma en una matriz, donde cada columna representa una palabra distinta del vocabulario y cada fila representa un documento. El valor en cada celda es la cantidad de veces que aparece esa palabra en ese documento.\n",
    "\n",
    "Por ejemplo, si tengo tres textos: \"hola mundo\", \"hola\" y \"mundo hola hola\"\n",
    "\n",
    "El `CountVectorizer` generará una matriz como esta:\n",
    "\n",
    "| | hola | mundo |\n",
    "|------|------|-------|\n",
    "| doc1 | 1 | 1 | \n",
    "| doc2 | 1 | 0 |\n",
    "| doc3 | 2 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99575c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del set de entrenamiento: (1283, 11191) \n",
      "(Significa que tiene 1283 documentos, 80.0% del total)\n",
      "Forma del set de testeo: (321, 11191) \n",
      "(Significa que tiene 321 documentos, 20.0% del total)\n",
      "Distribución de clases para 'veracity':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cantidad</th>\n",
       "      <th>Porcentaje</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veracity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mixture of true and false</th>\n",
       "      <td>209</td>\n",
       "      <td>13.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mostly false</th>\n",
       "      <td>82</td>\n",
       "      <td>5.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mostly true</th>\n",
       "      <td>1249</td>\n",
       "      <td>77.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no factual content</th>\n",
       "      <td>64</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Cantidad  Porcentaje\n",
       "veracity                                       \n",
       "mixture of true and false       209       13.03\n",
       "mostly false                     82        5.11\n",
       "mostly true                    1249       77.87\n",
       "no factual content               64        3.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases para 'orientation':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cantidad</th>\n",
       "      <th>Porcentaje</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orientation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>252</td>\n",
       "      <td>15.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mainstream</th>\n",
       "      <td>822</td>\n",
       "      <td>51.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>530</td>\n",
       "      <td>33.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Cantidad  Porcentaje\n",
       "orientation                      \n",
       "left              252       15.71\n",
       "mainstream        822       51.25\n",
       "right             530       33.04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separar en conjuntos de entrenamiento y testeo (80%/20%)\n",
    "X = df[\"mainText\"]\n",
    "y = df[\"veracity\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# test_size=0.2: 20% de los datos para testeo\n",
    "# random_state=42: el numero es arbitrario, el objetivo es que el resultado sea reproducible\n",
    "\n",
    "# Preprocesamiento con CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english', # Elijo ingles como idioma\n",
    "    max_df=0.6,    # Elimino palabras que aparecen en más del 60% de los documentos\n",
    "    min_df=3       # Eliminp palabras que aparecen en menos de 3 documentos\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "n_total = len(df)\n",
    "n_train = X_train_vec.shape[0]\n",
    "n_test = X_test_vec.shape[0]\n",
    "\n",
    "pct_train = n_train / n_total * 100\n",
    "pct_test = n_test / n_total * 100\n",
    "\n",
    "print(f\"Forma del set de entrenamiento: {X_train_vec.shape} \"\n",
    "      f\"\\n(Significa que tiene {n_train} documentos, {pct_train:.1f}% del total)\")\n",
    "print(f\"Forma del set de testeo: {X_test_vec.shape} \"\n",
    "      f\"\\n(Significa que tiene {n_test} documentos, {pct_test:.1f}% del total)\")\n",
    "\n",
    "# Funcion para mostrar la distribución de clases\n",
    "def show_distribution(df, column, name=None):\n",
    "    \"\"\"\n",
    "    Muestra la distribución de clases de una columna de un DataFrame.\n",
    "    \"\"\"\n",
    "    counts = df[column].value_counts().sort_index()\n",
    "    percent = df[column].value_counts(normalize=True).sort_index() * 100\n",
    "    distribucion = pd.DataFrame({\n",
    "        \"Cantidad\": counts,\n",
    "        \"Porcentaje\": percent.round(2)\n",
    "    })\n",
    "    if name is None:\n",
    "        name = column\n",
    "    print(f\"Distribución de clases para '{name}':\")\n",
    "    display(distribucion)\n",
    "\n",
    "show_distribution(df, \"veracity\")\n",
    "show_distribution(df, \"orientation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52660de",
   "metadata": {},
   "source": [
    "Se verifica que el set de entrenamiento tiene 1283 documentos y el de testeo 321 documentos, de un total de 1604 documentos, por lo que cumple el ratio de 80/20. \n",
    "\n",
    "Además, se analizó la distribución de las clases de veracidad y orientación. Por ejemplo, la mayoría de los textos (77.87%) son \"mostly true\" y la mayoría de las orientaciones (51.25%) son \"mainstream\".\n",
    "\n",
    "Se nota que los valores para veracidad son más desiguales que los para orientación, lo cual podría indicar que hay un sesgo en el dataset y que los datos pueden generar problemas en los siguientes resultados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ef365",
   "metadata": {},
   "source": [
    "\n",
    "## (b) Entrenamiento:\n",
    "\n",
    "Implementar un MNB de $\\alpha = (1, 1, \\cdots, 1)$ que prediga la veracidad de un artículo a partir de su texto principal (pre-procesado). El código debe estar estructurado de la siguiente manera:\n",
    "\n",
    "```python\n",
    "class MNB:\n",
    "    # Inicializar atributos y declarar hiperparámetros\n",
    "    def __init__(self,...):\n",
    "        ...\n",
    "\n",
    "    # Etapa de entrenamiento\n",
    "    def fit(self,X,y):\n",
    "        ...\n",
    "\n",
    "    # Etapa de testeo soft\n",
    "    def predict_proba(self,X):\n",
    "        ...\n",
    "\n",
    "    # Etapa de testeo hard (no repetir código)\n",
    "    def predict(self,X):\n",
    "        ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55069816",
   "metadata": {},
   "source": [
    "\n",
    "`MNB: Multinomial Naive Bayes`\n",
    "\n",
    "El modelo Multinomial Naive Bayes asume que las palabras de un documento son independientes entre sí; es decir, la aparición de una palabra no afecta la probabilidad de aparición de otra (de ahí el término \"Naive\"). El objetivo es modelar la distribución multinomial de las palabras en los diferentes documentos del dataset.\n",
    "\n",
    "**Teorema de Bayes**\n",
    "\n",
    "El teorema de Bayes nos dice que:\n",
    "\n",
    "$$ P(\\text{clase} \\mid \\text{documento}) = \\frac{P(\\text{documento} \\mid \\text{clase}) \\times P(\\text{clase})}{P(\\text{documento})} $$\n",
    "\n",
    "Para clasificar, buscamos la clase que maximiza esta probabilidad:\n",
    "\n",
    "$$ \\text{clase}_{\\text{predicha}} = \\arg\\max{\\left[P(\\text{clase} \\mid \\text{documento})\\right]} $$\n",
    "\n",
    "Como $P(\\text{documento})$ es constante para todas las clases, se puede omitir en la maximización:\n",
    "\n",
    "$$ \\text{clase}_{\\text{predicha}} = \\arg\\max{\\left[P(\\text{documento} \\mid \\text{clase}) \\times P(\\text{clase})\\right]} $$\n",
    "\n",
    "Aplicando logaritmos (para mayor estabilidad numérica):\n",
    "\n",
    "$$ \\text{clase}_{\\text{predicha}} = \\arg\\max{\\left[ \\log P(\\text{clase}) + \\log P(\\text{documento} \\mid \\text{clase}) \\right]} $$\n",
    "\n",
    "En el caso del modelo multinomial, la verosimilitud del documento dada la clase se calcula como:\n",
    "\n",
    "$$ \\log P(\\text{documento} \\mid \\text{clase}) = \\sum_{i} \\text{count}(\\text{palabra}_i) \\times \\log P(\\text{palabra}_i \\mid \\text{clase}) $$\n",
    "\n",
    "Para realizar el MNB, se siguen los siguientes pasos:\n",
    "\n",
    "1. fit(X, y): Aprende las probabilidades a partir de los datos de entrenamiento:\n",
    "    - Calcula la probabilidad de cada clase (P(clase), frecuencia de cada clase).\n",
    "    - Calcula la probabilidad de cada palabra dado cada clase (P(palabra|clase)) usando suavizado de Laplace:\n",
    "    $$ P(\\text{palabra}_i | \\text{clase}) = \\frac{\\text{count}(\\text{palabra}_i, \\text{clase}) + \\alpha}{\\sum_j \\text{count}(\\text{palabra}_j, \\text{clase}) + V \\cdot \\alpha} $$ \n",
    "    donde $V$ es el tamaño del vocabulario.\n",
    "\n",
    "2. predict_proba(X): Para cada documento, calcula la probabilidad (o log-probabilidad) de que pertenezca a cada clase usando la fórmula de Bayes. Se suele trabajar en log-probabilidades para evitar underflow numérico:\n",
    "$$ \\log P(\\text{clase}) + \\sum_{i} \\text{count}(\\text{palabra}_i) \\cdot \\log P(\\text{palabra}_i | \\text{clase}) $$\n",
    "\n",
    "3. predict(X): Devuelve la clase con mayor probabilidad para cada documento (usa los resultados de predict_proba).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba91db",
   "metadata": {},
   "source": [
    "\n",
    "## (c) Inferencia\n",
    "\n",
    "Implementar un método a la clase anterior que calcule el *accuracy* y la *Macro-F1*. Evaluar dichas métricas en el conjunto de testeo. ¿Por qué dan tan diferentes? Para el cálculo de la F1 debe considerar el caso de *precision* y *recall* nulas.\n",
    "\n",
    "**Métricas**\n",
    "\n",
    "- Accuracy: Proporción de documentos correctamente clasificados.\n",
    "- Macro-F1: F1-score promedio por clase, sin ponderar por cantidad de ejemplos por clase.\n",
    "\n",
    "Para cada clase, calcula:\n",
    "\n",
    "- Precision = $\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$ Mide la calidad de las predicciones positivas \n",
    "- Recall = $\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$ Mide la capacidad del modelo para encontrar todos los positivos reales \n",
    "- F1 = $\\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ Combina ambas métricas en un solo valor, es un promedio balanceado de ambas. Si Precision o Recall son nulos, el F1 para esa clase es 0. \n",
    "\n",
    "\n",
    "\n",
    "**¿Por qué pueden diferir tanto accuracy y Macro-F1?**\n",
    "- Accuracy mide el porcentaje total de aciertos, pero puede estar sesgado si hay clases desbalanceadas (por ejemplo, si una clase es muy frecuente).\n",
    "- Macro-F1 promedia el F1 de cada clase, dándole igual peso a todas, incluso a las minoritarias. Si el modelo ignora clases poco frecuentes, la Macro-F1 será baja aunque el accuracy sea alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06994968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6791277258566978\n",
      "Macro-F1: 0.3717473328129066\n",
      "\n",
      "Métricas por clase:\n",
      "mixture of true and false:\n",
      "  Precision: 0.2297\n",
      "  Recall: 0.3542\n",
      "  F1-score: 0.2787\n",
      "mostly false:\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.0588\n",
      "  F1-score: 0.1111\n",
      "mostly true:\n",
      "  Precision: 0.8182\n",
      "  Recall: 0.8049\n",
      "  F1-score: 0.8115\n",
      "no factual content:\n",
      "  Precision: 0.5000\n",
      "  Recall: 0.2000\n",
      "  F1-score: 0.2857\n"
     ]
    }
   ],
   "source": [
    "class MNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.asarray(y)\n",
    "        self.classes_, class_counts = np.unique(y, return_counts=True)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "        self.n_features_ = n_features\n",
    "\n",
    "        self.class_log_prior_ = np.log(class_counts) - np.log(class_counts.sum())\n",
    "\n",
    "        feature_count = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        for idx, c in enumerate(self.classes_):\n",
    "            feature_count[idx, :] = X[y == c].sum(axis=0)\n",
    "\n",
    "        smoothed_fc = feature_count + self.alpha\n",
    "        smoothed_cc = smoothed_fc.sum(axis=1, keepdims=True)\n",
    "        self.feature_log_prob_ = np.log(smoothed_fc) - np.log(smoothed_cc)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return jll\n",
    "\n",
    "    def predict(self, X):\n",
    "        jll = self.predict_proba(X)\n",
    "        idx = np.argmax(jll, axis=1)\n",
    "        return self.classes_[idx]\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        return (X @ self.feature_log_prob_.T) + self.class_log_prior_\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "    def macro_f1(self, X, y):\n",
    "        _, _, f1s = self.per_class_metrics(X, y)\n",
    "        return np.mean(f1s)\n",
    "\n",
    "    def per_class_metrics(self, X, y):\n",
    "        \"\"\"\n",
    "        Devuelve arrays de precision, recall y F1 para cada clase.\n",
    "        \"\"\"\n",
    "        y_true = np.asarray(y)\n",
    "        y_pred = self.predict(X)\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "        for c in self.classes_:\n",
    "            tp = np.sum((y_pred == c) & (y_true == c))\n",
    "            fp = np.sum((y_pred == c) & (y_true != c))\n",
    "            fn = np.sum((y_pred != c) & (y_true == c))\n",
    "            if tp + fp == 0:\n",
    "                precision = 0.0\n",
    "            else:\n",
    "                precision = tp / (tp + fp)\n",
    "            if tp + fn == 0:\n",
    "                recall = 0.0\n",
    "            else:\n",
    "                recall = tp / (tp + fn)\n",
    "            if precision + recall == 0:\n",
    "                f1 = 0.0\n",
    "            else:\n",
    "                f1 = 2 * precision * recall / (precision + recall)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "        return np.array(precisions), np.array(recalls), np.array(f1_scores)\n",
    "\n",
    "    def classification_report(self, X, y):\n",
    "        \"\"\"\n",
    "        Devuelve un diccionario con accuracy, macro_f1, y arrays de precisión, recall y F1 por clase.\n",
    "        \"\"\"\n",
    "        acc = self.accuracy(X, y)\n",
    "        macro_f1 = self.macro_f1(X, y)\n",
    "        precisions, recalls, f1_scores = self.per_class_metrics(X, y)\n",
    "        return {\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"precisions\": precisions,\n",
    "            \"recalls\": recalls,\n",
    "            \"f1_scores\": f1_scores,\n",
    "            \"classes\": self.classes_\n",
    "        }\n",
    "\n",
    "def ensure_dense(X):\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        return X.toarray()\n",
    "    return np.array(X)\n",
    "\n",
    "mnb = MNB(alpha=1)\n",
    "X_train_dense = ensure_dense(X_train_vec)\n",
    "X_test_dense = ensure_dense(X_test_vec)\n",
    "mnb.fit(X_train_dense, y_train)\n",
    "\n",
    "report = mnb.classification_report(X_test_dense, y_test)\n",
    "print(\"Accuracy:\", report[\"accuracy\"])\n",
    "print(\"Macro-F1:\", report[\"macro_f1\"])\n",
    "print(\"\\nMétricas por clase:\")\n",
    "for i, class_label in enumerate(report[\"classes\"]):\n",
    "    print(f\"{class_label}:\")\n",
    "    print(f\"  Precision: {report['precisions'][i]:.4f}\")\n",
    "    print(f\"  Recall: {report['recalls'][i]:.4f}\")\n",
    "    print(f\"  F1-score: {report['f1_scores'][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b5c4e",
   "metadata": {},
   "source": [
    "📊 **Análisis de los resultados:**\n",
    "\n",
    "**Accuracy: 67.9% vs Macro-F1: 37.2%**: La gran diferencia se debe al desbalance de clases y al rendimiento desigual por clase\n",
    "\n",
    "**Métricas por clase**\n",
    "\n",
    "- **mostly true**:  ⭐ Clase dominante y bien predicha\n",
    "  - Precision: 0.82 (de los que predijo como esta clase, 82% eran correctos)\n",
    "  - Recall: 0.80 (de los reales de esta clase, 80% fueron encontrados)\n",
    "  - F1: 0.81 (excelente, el modelo predice bien esta clase)\n",
    "\n",
    "- **mostly false**: 🚨 Problema crítico\n",
    "  - Precision: 1.00 (cuando predice esta clase, nunca se equivoca)\n",
    "  - Recall: 0.06 (¡pero casi nunca la predice! Solo 6% de los reales fueron encontrados)\n",
    "  - F1: 0.11 (muy bajo)\n",
    "\n",
    "- **no factual content**: ⚠️ Rendimiento medio\n",
    "  - Precision: 0.50\n",
    "  - Recall: 0.20\n",
    "  - F1: 0.29 (mejor que dar un valor random, pero bajo)\n",
    "\n",
    "- **mixture of true and false**: ⚠️ Rendimiento medio / bajo\n",
    "  - Precision: 0.23 \n",
    "  - Recall: 0.35 \n",
    "  - F1: 0.28 (bajo)\n",
    "\n",
    "\n",
    "**¿Por qué tanta diferencia entre Accuracy y Macro-F1?**\n",
    "\n",
    "Accuracy está dominado por la clase \"mostly true\", que el modelo predice muy bien y probablemente es la clase mayoritaria.\n",
    "Macro-F1 penaliza fuertemente el mal desempeño en las clases minoritarias (\"mostly false\", \"mixture of true and false\", \"no factual content\").\n",
    "\n",
    " Aunque el accuracy global sea alto, si el modelo ignora o predice mal las clases poco frecuentes, Macro-F1 baja mucho.\n",
    "\n",
    "**Conclusión:**\n",
    "El modelo parece ser bueno para la clase mayoritaria, pero malo detectando las minoritarias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da9a63a",
   "metadata": {},
   "source": [
    "\n",
    "## (d) Orientación:\n",
    "\n",
    "Repetir el ejercicio pero para clasificar la orientación política del portal donde fue publicada la noticia (izquierda, derecha o mainstream) a partir del texto principal preprocesado. ¿Siguen siendo válidas las conclusiones extraídas anteriormente? Justificar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8936d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.822429906542056\n",
      "Macro-F1: 0.7791249491249491\n",
      "\n",
      "Métricas por clase:\n",
      "left:\n",
      "  Precision: 0.6735\n",
      "  Recall: 0.5893\n",
      "  F1-score: 0.6286\n",
      "mainstream:\n",
      "  Precision: 0.8688\n",
      "  Recall: 0.9145\n",
      "  F1-score: 0.8910\n",
      "right:\n",
      "  Precision: 0.8214\n",
      "  Recall: 0.8142\n",
      "  F1-score: 0.8178\n"
     ]
    }
   ],
   "source": [
    "# Cambia la variable objetivo\n",
    "X = df[\"mainText\"]\n",
    "y = df[\"orientation\"]\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorización\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_df=0.6,\n",
    "    min_df=3\n",
    ")\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_dense = ensure_dense(X_train_vec)\n",
    "X_test_dense = ensure_dense(X_test_vec)\n",
    "\n",
    "mnb = MNB(alpha=1)\n",
    "mnb.fit(X_train_dense, y_train)\n",
    "report = mnb.classification_report(X_test_dense, y_test)\n",
    "\n",
    "print(\"Accuracy:\", report[\"accuracy\"])\n",
    "print(\"Macro-F1:\", report[\"macro_f1\"])\n",
    "print(\"\\nMétricas por clase:\")\n",
    "for i, class_label in enumerate(report[\"classes\"]):\n",
    "    print(f\"{class_label}:\")\n",
    "    print(f\"  Precision: {report['precisions'][i]:.4f}\")\n",
    "    print(f\"  Recall: {report['recalls'][i]:.4f}\")\n",
    "    print(f\"  F1-score: {report['f1_scores'][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8539b0",
   "metadata": {},
   "source": [
    "📊 **Análisis de los resultados:**\n",
    "\n",
    "- Accuracy (82%): Clasifica correctamente el 82%. Es un valor alto, indicando buen rendimiento general.\n",
    "- Macro-F1 (78%): Tiene mejor rendimiento en ambas clases, no solo en las mayoritarias. Ademas, se parece al valor del accuracy.\n",
    "- Por clase:\n",
    "  - mainstream: ⭐ Mejor clase. El modelo es muy bueno (F1: 0.89, precision y recall altos), lo que suele ser esperable si es la clase mayoritaria.\n",
    "  - right: ✅ Desempeño parecido (F1: 0.82).\n",
    "  - left: ⚠️ El desempeño es menor (F1: 0.63), pero sigue siendo aceptable. Posiblemente menos ejemplos o más difícil de distinguir\n",
    "\n",
    "**Comparación de resultados**\n",
    "- Problema anterior (veracidad):\n",
    "  - Accuracy: 67.9% vs Macro-F1: 37.2% (diferencia de 30.7%)\n",
    "  - Rendimiento muy desigual entre clases\n",
    "- Problema actual (orientación política):\n",
    "  - Accuracy: 82.2% vs Macro-F1: 77.9% (diferencia de solo 4.3%)\n",
    "  - Rendimiento mucho más equilibrado\n",
    "\n",
    "**¿Por qué funciona mejor?**\n",
    "Porque las clases están más balanceadas. La orientación política parece tener una distribución más equilibrada, lo cual coincide con lo visto en el punto a). Esto puede ser porque hay menos ambigüedad, la orientación política puede ser más fácil de detectar que la veracidad de las noticias.\n",
    "\n",
    "**Observaciones importantes**:\n",
    "- Macro-F1 cercano a Accuracy: Indica que el modelo funciona bien en todas las clases\n",
    "- Rendimiento consistente: Ninguna clase está siendo completamente ignorada. (en veracidad, \"mostly false\" nunca la detectaba)\n",
    "- Mejora significativa: De 37.2% a 77.9% en Macro-F1 (¡más del doble!)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-taller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
